# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

"""Type stubs for Fluss Python bindings."""

from typing import Dict, List, Optional

import pandas as pd
import pyarrow as pa

class Config:
    def __init__(self, config: Dict[str, str]) -> None: ...

class FlussConnection:
    @staticmethod
    async def connect(config: Config) -> FlussConnection: ...
    async def get_admin(self) -> FlussAdmin: ...
    async def get_table(self, table_path: TablePath) -> FlussTable: ...
    async def close(self) -> None: ...

class FlussAdmin:
    async def create_table(
        self,
        table_path: TablePath,
        table_descriptor: TableDescriptor,
        ignore_if_exists: bool = False,
    ) -> None: ...
    async def get_table(self, table_path: TablePath) -> TableInfo: ...
    async def get_latest_lake_snapshot(self, table_path: TablePath) -> LakeSnapshot: ...

class FlussTable:
    async def new_append_writer(self) -> AppendWriter: ...
    def new_log_scanner(self) -> LogScanner: ...
    def new_log_scanner_sync(self) -> LogScanner: ...

class AppendWriter:
    def write_arrow(self, table: pa.Table) -> None: ...
    def write_arrow_batch(self, batch: pa.RecordBatch) -> None: ...
    def flush(self) -> None: ...
    def close(self) -> None: ...

class LogScanner:
    def subscribe(
        self, snapshot_id: Optional[int], start_offset: Optional[int]
    ) -> None: ...
    def to_pandas(self) -> pd.DataFrame: ...
    def to_arrow(self) -> pa.Table: ...
    def __iter__(self) -> LogScannerIterator: ...

class LogScannerIterator:
    def __iter__(self) -> LogScannerIterator: ...
    def __next__(self) -> pa.RecordBatch: ...

class Schema:
    def __init__(self, schema: pa.Schema) -> None: ...

class TableDescriptor:
    def __init__(
        self, schema: Schema, properties: Optional[Dict[str, str]] = None
    ) -> None: ...

class TablePath:
    def __init__(self, database: str, table: str) -> None: ...
    def database(self) -> str: ...
    def table(self) -> str: ...

class TableInfo:
    @property
    def table_id(self) -> int: ...
    @property
    def schema_id(self) -> int: ...
    @property
    def created_time(self) -> int: ...
    def get_primary_keys(self) -> List[str]: ...

class DatabaseInfo:
    @property
    def database_name(self) -> str: ...
    @property
    def created_time(self) -> int: ...

class LakeSnapshot:
    def __init__(self, snapshot_id: int) -> None: ...
    @property
    def snapshot_id(self) -> int: ...
    @property
    def table_buckets_offset(self) -> Dict[TableBucket, int]: ...
    def get_bucket_offset(self, bucket: TableBucket) -> Optional[int]: ...
    def get_table_buckets(self) -> List[TableBucket]: ...

class TableBucket:
    def __init__(self, table_id: int, bucket: int) -> None: ...
    @staticmethod
    def with_partition(
        table_id: int, partition_id: int, bucket: int
    ) -> TableBucket: ...
    @property
    def table_id(self) -> int: ...
    @property
    def bucket_id(self) -> int: ...
    @property
    def partition_id(self) -> Optional[int]: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...

__version__: str
